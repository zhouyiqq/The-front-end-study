{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置字符集，防止中文乱码\n",
    "mpl.rcParams['font.sans-serif'] = [u'simHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 设置在jupyter中matplotlib的显示情况（表示不是嵌入显示）\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建训练数据集\n",
    "#假设训练学习一个线性函数y = 2.33x\n",
    "EXAMPLE_NUM = 100#训练总数\n",
    "BATCH_SIZE = 10#mini_batch训练集大小\n",
    "TRAIN_STEP = 150#训练次数\n",
    "LEARNING_RATE = 0.0001#学习率\n",
    "X_INPUT = np.arange(EXAMPLE_NUM) * 0.1#生成输入数据X\n",
    "Y_OUTPUT_CORRECT = 5 * X_INPUT#生成训练正确输出数据\n",
    "\n",
    "##构造训练的函数\n",
    "def train_func(X, K):\n",
    "    result = K * X\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "#BGD\n",
    "#参数初始化值\n",
    "k_BGD = 0.0\n",
    "#记录迭代数据用于作图\n",
    "k_BGD_RECORD = []\n",
    "for step in range(TRAIN_STEP):\n",
    "    SUM_BGD = 0\n",
    "    for index in range(len(X_INPUT)):\n",
    "        ###损失函数J(K)=1/(2m)*sum(KX-y_true)^2\n",
    "        ###J(K)的梯度 = (KX-y_true)*X\n",
    "        SUM_BGD += (train_func(X_INPUT[index], k_BGD) - Y_OUTPUT_CORRECT[index]) * X_INPUT[index]\n",
    "    ###这里实际上要对SUM_BGD求均值也就是要乘上个1/m 但是 LEARNING_RATE*1/m1 还是一个常数 所以这里就直接用一个常数表示\n",
    "    k_BGD -= LEARNING_RATE * SUM_BGD\n",
    "    k_BGD_RECORD.append(k_BGD)\n",
    "# k_BGD_RECORD \n",
    "print(len(k_BGD_RECORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "k_SGD = 0.0\n",
    "k_SGD_RECORD = []\n",
    "for step in range(TRAIN_STEP):\n",
    "    index = np.random.randint(len(X_INPUT))\n",
    "    SUM_SGD = (train_func(X_INPUT[index], k_SGD) - Y_OUTPUT_CORRECT[index]) * X_INPUT[index]\n",
    "    k_SGD -= LEARNING_RATE * SUM_SGD\n",
    "    k_SGD_RECORD.append(k_SGD)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MBGD\n",
    "k_MBGD = 0.0\n",
    "k_MBGD_RECORD = []\n",
    "for step in range(TRAIN_STEP):\n",
    "    SUM_MBGD = 0\n",
    "    index_start = np.random.randint(len(X_INPUT) - BATCH_SIZE)\n",
    "    for index in np.arange(index_start, index_start+BATCH_SIZE):\n",
    "        SUM_MBGD += (train_func(X_INPUT[index], k_MBGD) - Y_OUTPUT_CORRECT[index]) * X_INPUT[index]\n",
    "    k_MBGD -= LEARNING_RATE * SUM_MBGD\n",
    "    k_MBGD_RECORD.append(k_MBGD)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#作图\n",
    "plt.plot(np.arange(TRAIN_STEP), np.array(k_BGD_RECORD), label='BGD')\n",
    "plt.plot(np.arange(TRAIN_STEP), k_SGD_RECORD, label='SGD')\n",
    "plt.plot(np.arange(TRAIN_STEP), k_MBGD_RECORD, label='MBGD')\n",
    "plt.legend()\n",
    "plt.ylabel('K')\n",
    "plt.xlabel('step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "k_SGD = 0.0\n",
    "k_SGD_RECORD = []\n",
    "for step in range(TRAIN_STEP*20):\n",
    "    index = np.random.randint(len(X_INPUT))\n",
    "    SUM_SGD = (train_func(X_INPUT[index], k_SGD) - Y_OUTPUT_CORRECT[index]) * X_INPUT[index]\n",
    "    k_SGD -= LEARNING_RATE * SUM_SGD\n",
    "    k_SGD_RECORD.append(k_SGD)\n",
    "    \n",
    "# # 设置在jupyter中matplotlib的显示情况（表示不是嵌入显示）\n",
    "# %matplotlib tk\n",
    "\n",
    "plt.plot(np.arange(TRAIN_STEP*20), k_SGD_RECORD, label='SGD')\n",
    "plt.legend()\n",
    "plt.ylabel('K')\n",
    "plt.xlabel('step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "BGD：可以在迭代步骤上可以快速接近最优解，但是其时间消耗相对其他两种是最大的，因为每一次更新都需要遍历完所有数据。\n",
    "\n",
    "SGD：参数更新是最快的，因为每遍历一个数据都会做参数更新，但是由于没有遍历完所有数据，所以其路线不一定是最佳路线，甚至可能会反方向巡迹，不过其整体趋势是往最优解方向行进的，随机速度下降还有一个好处是有一定概率跳出局部最优解，而BGD会直接陷入局部最优解。\n",
    "\n",
    "MBGD：以上两种都是MBGD的极端，MBGD是中庸的选择，保证参数更新速度的前提下，用过小批量又增加了其准备度，所以大多数的梯度下降算法中都会使用到小批量梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
